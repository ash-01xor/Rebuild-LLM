{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up GPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,\n",
    "                 context_length,dropout,\n",
    "                 num_heads, qkv_bias=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out,d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length,context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        b, num_tokens , d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Splitting the matrix by adding num_heads\n",
    "        keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "\n",
    "\n",
    "        # Converting the shape\n",
    "        # b,num_tokens,num_heads,head_dim = b,num_heads,num_tokens,head_dim\n",
    "        # crucial for aligning the keys and values in multiple heads\n",
    "        keys = keys.transpose(1,2) \n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(2,3) # dot product for each head\n",
    "        mask_bool = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "        attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1]**0.5 , dim = -1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # b , num_token , n_heads , head_dim\n",
    "        context_vectors = (attention_weights @ values).transpose(1,2) \n",
    "\n",
    "        # combines the heads \n",
    "        context_vectors = context_vectors.contiguous().view(\n",
    "            b,num_tokens,self.d_out\n",
    "        )\n",
    "        context_vectors = self.out_proj(context_vectors)\n",
    "        return context_vectors\n",
    "    \n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out= cfg[\"emb_dim\"],\n",
    "            context_length= cfg[\"context_length\"],\n",
    "            num_heads= cfg[\"n_heads\"],\n",
    "            dropout= cfg[\"drop_rate\"],\n",
    "            qkv_bias= cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])   \n",
    "\n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)  \n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + residual\n",
    "        return x\n",
    "    \n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1,keepdim=True)\n",
    "        var = x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "    \n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return 0.5*x*(1+torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0/torch.pi))*\n",
    "            (x+0.044715+torch.pow(x,3))\n",
    "        ))\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"],cfg[\"emb_dim\"]*4),\n",
    "            GELU(),\n",
    "            nn.Linear(cfg[\"emb_dim\"]*4,cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for  _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self,in_idx):\n",
    "        batch_size , seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len,device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:The sky is Schnewhy ripple gratificationcularSizecampaign lies suggestions meet\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def generate_text(model,idx,max_new_tokens,context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:,-1,:] \n",
    "        probas = torch.softmax(logits,dim=-1)\n",
    "        idx_next = torch.argmax(probas,dim=-1,keepdim=True)\n",
    "        idx = torch.cat((idx,idx_next),dim=-1)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def text_to_token_ids(text,tokenizer):\n",
    "    encoded = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids,tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"The sky is\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context,tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(f\"Output text:{token_ids_to_text(token_ids,tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cells we have setup the architecture of the model and also the function for generation text. We can see that there is incoherant text being generated by the model. This is because the model is not trained on any data. We will now train the model on the data and then generate text using the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating loss for text generation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits,dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text generation process can be simplified as further:\n",
    "- Given a large corpus of vocabulary , we creating a mapping of the vocabulary to token IDs\n",
    "- we obtain a n-dimensional probability vector for each input token as specified in the configs\n",
    "- Identify the index position with highest probability values in each row\n",
    "- Obtain all the predicted token IDs as the index position with highest probability\n",
    "- Map the index position back to text via an inverse vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[24851],\n",
      "         [  406],\n",
      "         [40115]],\n",
      "\n",
      "        [[29716],\n",
      "         [40825],\n",
      "         [37022]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1: etti L HO\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model hasn't yet been trained on vocabulary we can see the outputs of not good quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([4.3679e-06, 2.1341e-05, 1.0216e-05])\n",
      "Text 2: tensor([1.2381e-05, 3.0002e-05, 6.1751e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.3412, -10.7549, -11.4916, -11.2994, -10.4142, -11.9950])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-11.3827)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.3827)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "#Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.3827)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(87790.6641)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross entropy loss is the difference between actual and predicted probability distribution of the model.\n",
    "- Perplexity is just the exponential of cross entropy\n",
    "- It also refers to the probability distribution predicted by the model and how well it matches the distribution of words present in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model of small dataset:\n",
    "\n",
    "Trying to train on the model on already existant dataset which was using in text-preprocessing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../Text-processing-basic/data.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of characters: 20479\n",
      "total number of tokens:5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(f\"total number of characters: {total_characters}\")\n",
    "print(f\"total number of tokens:{total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diving the dataset into training and validation4\n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18431, 2048)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0,len(token_ids)-max_length,stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx],self.target_ids[idx]\n",
    "    \n",
    "\n",
    "def create_dataloaderv1(txt,batch_size=4,max_length=256,\n",
    "                      stride=128,shuffle=True,drop_last=True,\n",
    "                      num_workers = 0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataset(txt,tokenizer,max_length,stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloaderv1(\n",
    "    train_data,\n",
    "    batch_size = 2,\n",
    "    max_length = GPT_CONFIG[\"context_length\"],\n",
    "    stride = GPT_CONFIG[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloaderv1(\n",
    "    val_data,\n",
    "    batch_size = 2,\n",
    "    max_length = GPT_CONFIG[\"context_length\"],\n",
    "    stride = GPT_CONFIG[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader data sizes\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "Validation loader data sizes\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train loader data sizes\")\n",
    "for x,y in train_loader:\n",
    "    print(x.shape,y.shape)\n",
    "\n",
    "print(f\"Validation loader data sizes\")\n",
    "for x,y in val_loader:\n",
    "    print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_batch(input_batch,target_batch,model,device):\n",
    "    \"\"\"\n",
    "    Function to calculate the loss for a single batch\n",
    "    \"\"\"\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1),target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader,model,device,num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches,len(data_loader))\n",
    "    for i,(input_batch,target_batch) in enumerate(data_loader):\n",
    "        if i<num_batches:\n",
    "            loss = calculate_loss_batch(\n",
    "                input_batch,target_batch,model,device\n",
    "            )\n",
    "            total_loss += loss\n",
    "        else:\n",
    "            break\n",
    "    return total_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:11.009906768798828\n",
      "Validation loss:11.048162460327148\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad(): #this is for disabling the gradient calculation\n",
    "    train_loss = calc_loss_loader(train_loader,model,device)\n",
    "    val_loss = calc_loss_loader(val_loader,model,device)\n",
    "print(f\"Training loss:{train_loss}\")\n",
    "print(f\"Validation loss:{val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model:\n",
    "\n",
    "The process of training the model is as follows:\n",
    "- iterate over the training epochs\n",
    "- iterate over the batches for each epoch\n",
    "- reset the gradients of loss from previous batch\n",
    "- calculate loss for current batch\n",
    "- backward pass to calculate loss gradients\n",
    "- update the model weights using loss gradients\n",
    "Inspect\n",
    "- the losses in training and validation set\n",
    "- sample text to see the quality of generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model,train_loader,val_loader,\n",
    "                       optimizer,device,num_epochs,eval_freq,\n",
    "                       eval_iter,start_context,tokenizer):\n",
    "    train_losses , val_losses , track_tokens_seen = [],[],[]\n",
    "    tokens_seen , global_step = 0,-1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch,target_batch in train_loader:\n",
    "            optimizer.zero_grad() #resets loss gradients in prev batch\n",
    "            loss = calculate_loss_batch(\n",
    "                input_batch,target_batch,model,device\n",
    "            )\n",
    "            loss.backward() # calculate loss gradients\n",
    "            optimizer.step() #update model weights\n",
    "            tokens_seen += input_batch.numel() \n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss , val_loss = evaluate_model(\n",
    "                    model,train_loader,val_loader,device,eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch:{epoch+1} (Step:{global_step:06d}): \"\n",
    "                      f\"Train loss:{train_loss:.4f}, \"\n",
    "                      f\"Validation loss:{val_loss:.4f}\")\n",
    "                \n",
    "        generate_print_sample(\n",
    "            model,tokenizer,device,start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,train_loader,val_loader,device,eval_iter):\n",
    "    model.eval() #dropout is disabled\n",
    "    with torch.no_grad(): #disable gradient tracking as  it not required during eval\n",
    "        train_loss = calc_loss_loader(train_loader,model,device,eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader,model,device,eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_print_sample(model,tokenizer,device,start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context,tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text(model,idx=encoded,\n",
    "                                  max_new_tokens=50,\n",
    "                                  context_size=context_size)\n",
    "    decoded_text = token_ids_to_text(token_ids,tokenizer)\n",
    "    print(f\"Generated text:\\n{decoded_text}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 (Step:000000): Train loss:10.5723, Validation loss:10.6358\n",
      "Epoch:1 (Step:000005): Train loss:9.2652, Validation loss:9.4483\n",
      "Generated text:\n",
      "The whole world is,,,, the,, the,,,, the, the,,, the the, the,,, the,, the,,,, the,,, the, the, the, the, the,, the,\n",
      "Epoch:2 (Step:000010): Train loss:8.6954, Validation loss:8.9277\n",
      "Epoch:2 (Step:000015): Train loss:8.1692, Validation loss:8.4476\n",
      "Generated text:\n",
      "The whole world is the, the, the, the, the, the, the.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch:3 (Step:000020): Train loss:7.6541, Validation loss:8.0169\n",
      "Epoch:3 (Step:000025): Train loss:7.1809, Validation loss:7.6020\n",
      "Generated text:\n",
      "The whole world is, the, the, the, the, the, the the.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch:4 (Step:000030): Train loss:6.7261, Validation loss:7.2775\n",
      "Epoch:4 (Step:000035): Train loss:6.2174, Validation loss:7.0114\n",
      "Generated text:\n",
      "The whole world is the.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch:5 (Step:000040): Train loss:5.8261, Validation loss:6.8582\n",
      "Generated text:\n",
      "The whole world is the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\" I had the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"--I, I had the, I had the I had the I had the the--I, and\n",
      "Epoch:6 (Step:000045): Train loss:5.4446, Validation loss:6.7269\n",
      "Epoch:6 (Step:000050): Train loss:5.1034, Validation loss:6.5778\n",
      "Generated text:\n",
      "The whole world is the of the.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch:7 (Step:000055): Train loss:4.6632, Validation loss:6.5370\n",
      "Epoch:7 (Step:000060): Train loss:4.3240, Validation loss:6.4581\n",
      "Generated text:\n",
      "The whole world is the of the.\n",
      "\n",
      "\n",
      "\"I--I--as the.\n",
      "\"I was the a, the, I had the to the.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"I of the, and I had been his I had\n",
      "Epoch:8 (Step:000065): Train loss:3.9930, Validation loss:6.4382\n",
      "Epoch:8 (Step:000070): Train loss:3.7443, Validation loss:6.3370\n",
      "Generated text:\n",
      "The whole world is the.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"--as the.\n",
      "\"I--and by a to me.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\" I had the donkey. I had been his.\n",
      "\n",
      "Epoch:9 (Step:000075): Train loss:3.5072, Validation loss:6.2915\n",
      "Epoch:9 (Step:000080): Train loss:3.1109, Validation loss:6.3152\n",
      "Generated text:\n",
      "The whole world is the I was not that I felt--I--I of the.\n",
      "\n",
      "\"I was a little that the\n",
      "\n",
      "\n",
      "\n",
      "\"--as, and his pictures.\n",
      "\n",
      "\n",
      "\"I had the donkey. \"There--I was.\n",
      "Epoch:10 (Step:000085): Train loss:2.8856, Validation loss:6.3731\n",
      "Generated text:\n",
      "The whole world is the I was one of the picture--I felt, and he had been in the: \"--as, in the I had to see a smile, on a _, as his pictures--as, the, and \"strong.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-4, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses , val_losses, tokens_seen = train_model_simple(\n",
    "    model,train_loader,val_loader,optimizer,device,\n",
    "    num_epochs=num_epochs,eval_freq=5,eval_iter=5,\n",
    "    start_context=\"The whole world is\",tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
