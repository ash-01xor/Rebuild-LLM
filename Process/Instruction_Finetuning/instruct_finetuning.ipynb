{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruct finetuning\n",
    "\n",
    "When a large language model is trained ,what it is essentially trained for is text completion. However model's are trained to follow instructions provided by us as prompts\n",
    "\n",
    "There are different stages for instruction finetuning a model.\n",
    "- Preparing a dataset\n",
    "- Finetuning a LLM\n",
    "- Evaluating the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "def download_and_load_file(file_path,url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "        with open(file_path, 'w',encoding='utf-8') as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path,\"r\",encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "    with open(file_path,\"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "\"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction: Name a synonym for 'exciting'.\n",
      "input: \n",
      "output: A synonym for 'exciting' is 'thrilling'.\n",
      "**************************************************\n",
      "instruction: Convert 5 kilograms to grams.\n",
      "input: \n",
      "output: 5 kilograms is 5000 grams.\n",
      "**************************************************\n",
      "instruction: Convert 3 liters to milliliters.\n",
      "input: \n",
      "output: 3 liters is 3000 milliliters.\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "def inspect_dataset(data,indexes):\n",
    "    for index in indexes:\n",
    "        print(\"\\n\".join(f\"{key}: {value}\" for key, value in data[index].items()))\n",
    "        print(\"*\"*50)\n",
    "\n",
    "indexes = [455,912,754]\n",
    "inspect_dataset(data,indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   From what we can see by inspecting the dataset, the dataset consists of simple basic instructions. Occasionally the input fields are empty as well. \n",
    "\n",
    "Instruction fine-tuning involves training a model on dataset where input-output pairs are provided. There are various prompt styles of instruct finetuning model.\n",
    "\n",
    "[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) was one of the initial models released and we would be following the same style of prompt for instruction finetuning through this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes as task. \"\n",
    "        f\"Write a response that approximately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = (\n",
    "        f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    )\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes as task. Write a response that approximately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Provide a word that rhymes with \"care.\"\n",
      "\n",
      "### Response:\n",
      "A word that rhymes with \"care\" is \"fare.\"\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[259])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[259]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 935\n",
      "Test data: 110\n",
      "Validation data: 55\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Train data:\", len(train_data))\n",
    "print(\"Test data:\", len(test_data))\n",
    "print(\"Validation data:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching the data:\n",
    "\n",
    "When dealing with this dataset, the input-output pairs are not of the same size and vary. Inorder to treat them the same way and also to make the training process efficient we get to batch the data. The steps needs to be done are:\n",
    "- tokenize the formatted data\n",
    "- use the pad tokens to make the input-output pairs of the same size\n",
    "- create target-IDs for training\n",
    "- finally exclude the padding tokens in loss function calculation by adding in placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self,data,tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(tokenizer.encode(full_text))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\",allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "        batch,\n",
    "        pad_token_id=50256,\n",
    "        device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Used to find the largest sequence in the batch and pad all sequences to that length.\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    input_list = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        input_list.append(inputs)\n",
    "\n",
    "    input_tensor = torch.stack(input_list).to(device)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0,1,2,3,4]\n",
    "inputs_2 = [5,6]\n",
    "inputs_3 = [7,8,9]\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are now able to format the data using a prompt template , encode them and adjust them based on the length.\n",
    "- Need to create target tokenid's for training and replace them with padding tokens with placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "        batch,\n",
    "        pad_token_id=50256,\n",
    "        device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Used to find the largest sequence in the batch for both input and target sequence.\n",
    "    Pad all sequences to that length .\n",
    "    Truncate the last token for inputs , while shifts a position for target sequences.\n",
    "    Returns input and target tensors.\n",
    "    \"\"\"\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    print(f\"batch_max_length: {batch_max_length}\")\n",
    "    input_list , target_list = [],[]\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id]* (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "        input_list.append(inputs)\n",
    "        target_list.append(targets)\n",
    "    \n",
    "    input_tensor = torch.stack(input_list).to(device)\n",
    "    target_tensor = torch.stack(target_list).to(device)\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_max_length: 6\n",
      "Inputs:tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "\n",
      "Targets:tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs,targets = custom_collate_draft_2(batch)\n",
    "print(f\"Inputs:{inputs}\\n\\nTargets:{targets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_batch(\n",
    "        batch,\n",
    "        pad_token_id = 50256,\n",
    "        ignore_index=-100,\n",
    "        allowed_max_length=None,\n",
    "        device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_list , target_list = [],[]\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item +[pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "        #print(f\"inputs:{inputs}\\n\\nTargets:{targets}\")\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "            #print(f\"Targets after masking:{targets}\")\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_list.append(inputs)\n",
    "        target_list.append(targets)\n",
    "\n",
    "    input_tensor = torch.stack(inputs_list).to(device)\n",
    "    target_tensor = torch.stack(target_list).to(device)\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "\n",
      "Targets:tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs,targets = custom_collate_batch(batch)\n",
    "print(f\"Inputs:{inputs}\\n\\nTargets:{targets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "        [[-1.0, 1.0],\n",
    "        [-0.5, 1.5]]\n",
    "        )\n",
    "targets_1 = torch.tensor([0, 1]) # Correct token indices to generate\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "    [-0.5, 1.5],\n",
    "    [-0.5, 1.5]]\n",
    "    )\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_batch,\n",
    "    allowed_max_length=1024,\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_dataset =  InstructionDataset(train_data,tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data,tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data,tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape:torch.Size([8, 63])\n",
      "Targets shape:torch.Size([8, 63])\n",
      "**************************************************\n",
      "Inputs shape:torch.Size([8, 69])\n",
      "Targets shape:torch.Size([8, 69])\n",
      "**************************************************\n",
      "Inputs shape:torch.Size([8, 71])\n",
      "Targets shape:torch.Size([8, 71])\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for i,(inputs,targets) in enumerate(train_loader):\n",
    "    if i==3:\n",
    "        break\n",
    "    i+=1\n",
    "    print(f\"Inputs shape:{inputs.shape}\")\n",
    "    print(f\"Targets shape:{targets.shape}\")\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a pretrained LLM\n",
    "\n",
    "Using the small version of GPT-2 , due to memory constrains if i get to download the medium sized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 13:09:47.337853: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748072387.413827   14554 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748072387.435494   14554 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-24 13:09:47.597692: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from previous_process import GPTModel,load_weights_into_gpt\n",
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes as task. Write a response that approximately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_process import generate , text_to_token_ids , token_ids_to_text\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active\n"
     ]
    }
   ],
   "source": [
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction Finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_process import calc_loss_loader , train_model_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the initial loss of training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.404801368713379\n",
      "Validation loss: 4.281529426574707\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(\n",
    "    train_loader, model, device, num_batches=5\n",
    "    )\n",
    "    val_loss = calc_loss_loader(\n",
    "    val_loader, model, device, num_batches=5\n",
    "    )\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.430, Val loss 0.738\n",
      "Ep 1 (Step 000050): Train loss 0.457, Val loss 0.802\n",
      "Ep 1 (Step 000100): Train loss 0.327, Val loss 0.759\n",
      "Below is an instruction that describes as task. Write a response that approximately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes as task. Write a response that approximately completes the request.  ### Input: What is the boiling point of ethanol in Celsius?\n",
      "Ep 2 (Step 000150): Train loss 0.318, Val loss 0.803\n",
      "Ep 2 (Step 000200): Train loss 0.283, Val loss 0.790\n",
      "Below is an instruction that describes as task. Write a response that approximately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes as task. Write a response that approximately completes the request.  ### Input: What is the capital of the United States? \n",
      "Ep 3 (Step 000250): Train loss 0.229, Val loss 0.843\n",
      "Ep 3 (Step 000300): Train loss 0.221, Val loss 0.837\n",
      "Below is an instruction that describes as task. Write a response that approximately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes as task. Write a response that approximately completes the request.  ### Instruction: Rewrite the sentence using a metaphor.  \n",
      "Ep 4 (Step 000350): Train loss 0.197, Val loss 0.842\n",
      "Ep 4 (Step 000400): Train loss 0.199, Val loss 0.859\n",
      "Ep 4 (Step 000450): Train loss 0.184, Val loss 0.842\n",
      "Below is an instruction that describes as task. Write a response that approximately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes as task. Write a response that approximately completes the request.  ### Input: What is the boiling point of water in Celsius?\n",
      "Ep 5 (Step 000500): Train loss 0.177, Val loss 0.847\n",
      "Ep 5 (Step 000550): Train loss 0.167, Val loss 0.859\n",
      "Below is an instruction that describes as task. Write a response that approximately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal every day by the chef is cooked by the chef.<|endoftext|>The following is an instruction that describes as task. Write a response that approximately completes the request.  ### Instruction: Rewrite the sentence using\n",
      "Training completed in 2.10 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=0.00005, weight_decay=0.1\n",
    "    )\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAEiCAYAAABTO2OcAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASolJREFUeJzt3Xd4FOXax/Hv7qaXTSCQBoQaSegdQxAVUIoiVZTD0WAXKSIKyAsiwkFQEVFBFAuoiCBVQIoBKRqp0iGEGookhJZK+j7vH0s2WWoSkp2U+3Ndc2Vn5tln7hlCfjuzU3RKKYUQQgghipVe6wKEEEKI8kACVwghhLABCVwhhBDCBiRwhRBCCBuQwBVCCCFsQAJXCCGEsAEJXCGEEMIGJHCFEEIIG5DAFUIIIWxAAleIEio6OhqdTsfevXu1LkUIUQQkcIUoRjqd7o7D+PHjtS5RCGEjdloXIERZFhMTY3m9cOFCxo0bR1RUlGWam5ubFmUJITQge7hCFCNfX1/L4OHhgU6ns4x7e3szbdo0qlatiqOjI02aNGHt2rW37Ss7O5vnn3+eoKAgzpw5A8Cvv/5Ks2bNcHJyolatWrz33ntkZWVZ3qPT6fjmm2/o2bMnLi4uBAYGsmLFCsv8q1ev0r9/fypXroyzszOBgYHMmTPntjUsXryYhg0b4uzsjJeXFx07diQlJcUy/5tvviE4OBgnJyeCgoL44osvrN5/9uxZ+vbti6enJxUrVqR79+5ER0db5g8YMIAePXowdepU/Pz88PLyYtCgQWRmZuZ7mwtRYikhhE3MmTNHeXh4WManTZumjEaj+vnnn9WRI0fUyJEjlb29vTp69KhSSqlTp04pQO3Zs0elpaWpnj17qqZNm6q4uDillFJbtmxRRqNRzZ07V504cUL9/vvvqkaNGmr8+PGWZQCqatWqav78+erYsWNq6NChys3NTV2+fFkppdSgQYNUkyZN1M6dO9WpU6dUeHi4WrFixS3rP3/+vLKzs1PTpk1Tp06dUvv371czZ85USUlJSiml5s2bp/z8/NSSJUvUyZMn1ZIlS1TFihXV3LlzlVJKZWRkqODgYPX888+r/fv3q8OHD6v//Oc/qm7duio9PV0ppVRYWJgyGo3q1VdfVZGRkWrlypXKxcVFzZ49u2j/MYTQgASuEDZyY+D6+/urSZMmWbVp2bKleu2115RSuYH7559/qg4dOqi2bduq+Ph4S9sOHTqo999/3+r9P/74o/Lz87OMA2rs2LGW8eTkZAWoNWvWKKWU6tatm3ruuefyVf8///yjABUdHX3L+bVr11bz58+3mjZx4kQVEhJiqa1u3brKZDJZ5qenpytnZ2e1bt06pZQ5cKtXr66ysrIsbZ588kn11FNP5atGIUoy+Q5XCA0kJiZy/vx5QkNDraaHhoayb98+q2n9+vWjatWq/PHHHzg7O1um79u3j4iICCZNmmSZlp2dTVpaGteuXcPFxQWARo0aWea7urpiNBqJi4sDYODAgfTu3Zvdu3fz6KOP0qNHD9q0aXPLmhs3bkyHDh1o2LAhnTp14tFHH6VPnz5UqFCBlJQUTpw4wQsvvMBLL71keU9WVhYeHh6Weo8fP467u7tVv2lpaZw4ccIyXr9+fQwGg2Xcz8+PAwcO3GFrClE6SOAKUcJ17dqVefPmsXXrVtq3b2+ZnpyczHvvvUevXr1ueo+Tk5Pltb29vdU8nU6HyWQCoEuXLpw+fZrVq1cTHh5Ohw4dGDRoEFOnTr2pT4PBQHh4OH///Te///47n3/+OWPGjGH79u2WcP/6669p3br1Te/Lqbd58+b89NNPN/VduXLlfNUrRGkmgSuEBoxGI/7+/kRERPDggw9apkdERNCqVSurtgMHDqRBgwY88cQT/Pbbb5b2zZo1Iyoqijp16txTLZUrVyYsLIywsDAeeOABRowYccvABXP4hYaGEhoayrhx46hevTrLli1j+PDh+Pv7c/LkSfr373/L9zZr1oyFCxfi7e2N0Wi8p5qFKI0kcIXQyIgRI3j33XepXbs2TZo0Yc6cOezdu/eWe4BDhgwhOzubxx9/nDVr1tC2bVvGjRvH448/TkBAAH369EGv17Nv3z4OHjzI//73v3zVMG7cOJo3b079+vVJT09n1apVBAcH37Lt9u3b2bBhA48++ije3t5s376dixcvWtq/9957DB06FA8PDzp37kx6ejq7du3i6tWrDB8+nP79+/PRRx/RvXt3JkyYQNWqVTl9+jRLly5l5MiRVK1atfAbU4hSQAJXCI0MHTqUhIQE3nzzTeLi4qhXrx4rVqwgMDDwlu2HDRuGyWSia9eurF27lk6dOrFq1SomTJjABx98gL29PUFBQbz44ov5rsHBwYHRo0cTHR2Ns7MzDzzwAAsWLLhlW6PRyJYtW5g+fTqJiYlUr16djz/+mC5dugDw4osv4uLiwkcffcSIESNwdXWlYcOGDBs2DAAXFxe2bNnCqFGj6NWrF0lJSVSpUoUOHTrIHq8oF3RKKaV1EUIIIURZJze+EEIIIWxAAlcIIYSwAQlcIYQQwgYkcIUQQggbkMAVQgghbEACVwghhLCBch+4M2fOpEaNGjg5OdG6dWt27NihdUkFMnnyZFq2bIm7uzve3t706NHD6nmrYL5X7aBBg/Dy8sLNzY3evXtz4cIFqzZnzpzhsccew8XFBW9vb0aMGGH1mDeATZs20axZMxwdHalTpw5z5869qZ6Stj2nTJmCTqezXAsK5W97/Pvvv/z3v//Fy8sLZ2dnGjZsyK5duyzzlVKMGzcOPz8/nJ2d6dixI8eOHbPq48qVK/Tv3x+j0YinpycvvPACycnJVm3279/PAw88gJOTE9WqVePDDz+8qZZFixYRFBSEk5MTDRs2ZPXq1cWz0reRnZ3NO++8Q82aNXF2dqZ27dpMnDiRvFdHluXtsWXLFrp164a/vz86nY7ly5dbzS9J656fWkodDR+coLkFCxYoBwcH9d1336lDhw6pl156SXl6eqoLFy5oXVq+derUSc2ZM0cdPHhQ7d27V3Xt2lUFBASo5ORkS5tXX31VVatWTW3YsEHt2rVL3X///apNmzaW+VlZWapBgwaqY8eOas+ePWr16tWqUqVKavTo0ZY2J0+eVC4uLmr48OHq8OHD6vPPP1cGg0GtXbvW0qakbc8dO3aoGjVqqEaNGqnXX3/dMr08bY8rV66o6tWrqwEDBqjt27erkydPqnXr1qnjx49b2kyZMkV5eHio5cuXq3379qknnnhC1axZU6WmplradO7cWTVu3Fht27ZN/fnnn6pOnTqqX79+lvkJCQnKx8dH9e/fXx08eFD9/PPPytnZWX311VeWNhEREcpgMKgPP/xQHT58WI0dO1bZ29urAwcO2GZjKKUmTZqkvLy81KpVq9SpU6fUokWLlJubm/r0008tbcry9li9erUaM2aMWrp0qQLUsmXLrOaXpHXPTy2lTbkO3FatWqlBgwZZxrOzs5W/v7+aPHmyhlXdm7i4OAWozZs3K6WUio+PV/b29mrRokWWNpGRkQpQW7duVUqZ/xPq9XoVGxtraTNr1ixlNBotzykdOXKkql+/vtWynnrqKdWpUyfLeEnanklJSSowMFCFh4erBx980BK45W17jBo1SrVt2/a2800mk/L19VUfffSRZVp8fLxydHRUP//8s1JKqcOHDytA7dy509JmzZo1SqfTqX///VcppdQXX3yhKlSoYNk+OcuuW7euZbxv377qscces1p+69at1SuvvHJvK1kAjz32mHr++eetpvXq1Uv1799fKVW+tseNgVuS1j0/tZRG5faQckZGBv/88w8dO3a0TNPr9XTs2JGtW7dqWNm9SUhIAKBixYoA/PPPP2RmZlqtZ1BQEAEBAZb13Lp1Kw0bNsTHx8fSplOnTiQmJnLo0CFLm7x95LTJ6aOkbc9Bgwbx2GOP3VRzedseK1asoEWLFjz55JN4e3vTtGlTvv76a8v8U6dOERsba1Wnh4cHrVu3ttoenp6etGjRwtKmY8eO6PV6tm/fbmnTrl07HBwcLG06depEVFQUV69etbS50zazhTZt2rBhwwaOHj0KmB8Z+Ndff1luT1netkdeJWnd81NLaVRuA/fSpUtkZ2db/VEF8PHxITY2VqOq7o3JZGLYsGGEhobSoEEDAGJjY3FwcMDT09Oqbd71jI2NveV2yJl3pzaJiYmkpqaWqO25YMECdu/ezeTJk2+aV962x8mTJ5k1axaBgYGsW7eOgQMHMnToUL7//nvLeuTUdbs6Y2Nj8fb2tppvZ2dHxYoVi2Sb2XJ7vP322zz99NMEBQVhb29P06ZNGTZsmOUJR+Vte+RVktY9P7WURvLwgjJk0KBBHDx4kL/++kvrUjRz9uxZXn/9dcLDw62eCVtemUwmWrRowfvvvw9A06ZNOXjwIF9++SVhYWEaV2d7v/zyCz/99BPz58+nfv367N27l2HDhuHv718ut4ewrXK7h1upUiUMBsNNZ6deuHABX19fjaoqvMGDB7Nq1So2btxo9ZgzX19fMjIyiI+Pt2qfdz19fX1vuR1y5t2pjdFoxNnZucRsz3/++Ye4uDiaNWuGnZ0ddnZ2bN68mc8++ww7Ozt8fHzK1fbw8/OjXr16VtOCg4M5c+YMkLs+d6rT19eXuLg4q/lZWVlcuXKlSLaZLbfHiBEjLHu5DRs25JlnnuGNN96wHA0pb9sjr5K07vmppTQqt4Hr4OBA8+bN2bBhg2WayWRiw4YNhISEaFhZwSilGDx4MMuWLeOPP/6gZs2aVvObN2+Ovb291XpGRUVx5swZy3qGhIRw4MABq/9I4eHhGI1Gyx/rkJAQqz5y2uT0UVK2Z4cOHThw4AB79+61DC1atKB///6W1+Vpe4SGht50mdjRo0epXr06ADVr1sTX19eqzsTERLZv3261PeLj4/nnn38sbf744w9MJhOtW7e2tNmyZQuZmZmWNuHh4dStW5cKFSpY2txpm9nCtWvX0Out/+wZDAZMJhNQ/rZHXiVp3fNTS6mk9VlbWlqwYIFydHRUc+fOVYcPH1Yvv/yy8vT0tDo7taQbOHCg8vDwUJs2bVIxMTGW4dq1a5Y2r776qgoICFB//PGH2rVrlwoJCVEhISGW+TmXwTz66KNq7969au3atapy5cq3vAxmxIgRKjIyUs2cOfOWl8GUxO2Z9yxlpcrX9tixY4eys7NTkyZNUseOHVM//fSTcnFxUfPmzbO0mTJlivL09FS//vqr2r9/v+revfstLwVp2rSp2r59u/rrr79UYGCg1aUg8fHxysfHRz3zzDPq4MGDasGCBcrFxeWmS0Hs7OzU1KlTVWRkpHr33XdtfllQWFiYqlKliuWyoKVLl6pKlSqpkSNHWtqU5e2RlJSk9uzZo/bs2aMANW3aNLVnzx51+vTpErfu+amltCnXgauUUp9//rkKCAhQDg4OqlWrVmrbtm1al1QgwC2HOXPmWNqkpqaq1157TVWoUEG5uLionj17qpiYGKt+oqOjVZcuXZSzs7OqVKmSevPNN1VmZqZVm40bN6omTZooBwcHVatWLatl5CiJ2/PGwC1v22PlypWqQYMGytHRUQUFBanZs2dbzTeZTOqdd95RPj4+ytHRUXXo0EFFRUVZtbl8+bLq16+fcnNzU0ajUT333HMqKSnJqs2+fftU27ZtlaOjo6pSpYqaMmXKTbX88ssv6r777lMODg6qfv366rfffiv6Fb6DxMRE9frrr6uAgADl5OSkatWqpcaMGWN1CUtZ3h4bN2685d+LsLAwpVTJWvf81FLayAPohRBCCBsot9/hCiGEELYkgSuEEELYgASuEEIIYQMSuEIIIYQNSOAKIYQQNiCBK4QQQtiABC6Qnp7O+PHjSU9P17oUzcm2sCbbw5psD2uyPXLJtrg7uQ4X8y3DPDw8SEhIwGg0al2OpmRbWJPtYU22hzXZHrlkW9yd7OEKIYQQNiCBK4QQQthAqX4eblZWFnv27MHHx+emJ4AURFJSEgD//vsviYmJRVVeqSTbwppsD2uyPazJ9shVnreFyWTiwoULNG3aFDu728dqqf4Od+fOnbRq1UrrMoQQQgh27NhBy5Ytbzu/VO/h+vj4AOaV9PPz07gaIYQQ5VFMTAytWrWyZNLtlOrAzTmM7OfnR9WqVTWuRgghRHl2t6825aQpIYQQwgYkcIUQQggbkMAVQgghbKBUf4crhBB3kp2dTWZmptZliFLO3t4eg8Fwz/1I4F4Xk5BK+OELPBtSQ+tShBD3SClFbGws8fHxWpciyghPT098fX3R6XSF7kMCF7iakkGnT7aQmJZFzUquPBBYWeuShBD3ICdsvb29cXFxuac/kqJ8U0px7do14uLiAO7pElQJXKCCqwM9m1bh+62neXvJAda90Q43R9k0QpRG2dnZlrD18vLSuhxRBjg7OwMQFxeHt7d3oQ8vy0lT143sHETVCs78G5/Kh2uPaF2OEKKQcr6zdXFx0bgSUZbk/D7dyzkBErjXuTra8UHvRgD8sPU0209e1rgiIcS9kMPIoigVxe+TBG4OpQhN3cSIBuYbcI9asp/UjGyNixJCCFFWSODm2P4VLHmBV69OJcBdT/Tla3z8e5TWVQkhRKHVqFGD6dOn57v9pk2b0Ol0xX5299y5c/H09CzWZZREErg5GvUFNx8Ml4/xY+31AHwbcYrdZ65qXJgQoqzT6XR3HMaPH1+ofnfu3MnLL7+c7/Zt2rQhJiYGDw+PQi1P3JkEbg6XivD4dACqH/mW4XWvohSMXLyftEw5tCyEKD4xMTGWYfr06RiNRqtpb731lqWtUoqsrKx89Vu5cuUCnTzm4OBwz9eaituTwM0rqCs07gcoBiV+TBVXOB6XzOd/HNO6MiFEGebr62sZPDw80Ol0lvEjR47g7u7OmjVraN68OY6Ojvz111+cOHGC7t274+Pjg5ubGy1btmT9+vVW/d54SFmn0/HNN9/Qs2dPXFxcCAwMZMWKFZb5Nx5Szjn0u27dOoKDg3Fzc6Nz587ExMRY3pOVlcXQoUPx9PTEy8uLUaNGERYWRo8ePQq0DWbNmkXt2rVxcHCgbt26/Pjjj5Z5SinGjx9PQEAAjo6O+Pv7M3ToUMv8L774gsDAQJycnPDx8aFPnz4FWratSODeqPMUcPfDcPUkP9VcB8CXm09y8N8EjQsTQhSWUoprGVk2H5RSRbYOb7/9NlOmTCEyMpJGjRqRnJxM165d2bBhA3v27KFz585069aNM2fO3LGf9957j759+7J//366du1K//79uXLlym3bX7t2jalTp/Ljjz+yZcsWzpw5Y7XH/cEHH/DTTz8xZ84cIiIiSExMZPny5QVat2XLlvH666/z5ptvcvDgQV555RWee+45Nm7cCMCSJUv45JNP+Oqrrzh27BjLly+nYcOGAOzatYuhQ4cyYcIEoqKiWLt2Le3atSvQ8m1F7u5wI2dPeOJz+KkPNY7/wOt1mvLpcW/eWrSPFYPb4mAnn1GEKG1SM7OpN26dzZd7eEInXByK5s/shAkTeOSRRyzjFStWpHHjxpbxiRMnsmzZMlasWMHgwYNv28+AAQPo168fAO+//z6fffYZO3bsoHPnzrdsn5mZyZdffknt2rUBGDx4MBMmTLDM//zzzxk9ejQ9e/YEYMaMGaxevbpA6zZ16lQGDBjAa6+9BsDw4cPZtm0bU6dO5eGHH+bMmTP4+vrSsWNH7O3tCQgIoFWrVgCcOXMGV1dXHn/8cdzd3alevTpNmzYt0PJtRdLjVgIfgabPAIqhSZ/g75zFkdgkvtx8QuvKhBDlVIsWLazGk5OTeeuttwgODsbT0xM3NzciIyPvuofbqFEjy2tXV1eMRqPltoW34uLiYglbMN/aMKd9QkICFy5csIQfgMFgoHnz5gVat8jISEJDQ62mhYaGEhkZCcCTTz5JamoqtWrV4qWXXmLZsmWW77EfeeQRqlevTq1atXjmmWf46aefuHbtWoGWbyuyh3s7nd6HExsxJJxmXs21tI98nM//OEan+r7U9XXXujohRAE42xs4PKGTJsstKq6urlbjb731FuHh4UydOpU6derg7OxMnz59yMjIuGM/9vb2VuM6nQ6TyVSg9kV5qDw/qlWrRlRUFOvXryc8PJzXXnuNjz76iM2bN+Pu7s7u3bvZtGkTv//+O+PGjWP8+PHs3LmzxF16JHu4t+NkhO4zAKh1aj6Da5wnM1sxYvE+srJv/8sphCh5dDodLg52Nh+K82zfiIgIBgwYQM+ePWnYsCG+vr5ER0cX2/JuxcPDAx8fH3bu3GmZlp2dze7duwvUT3BwMBEREVbTIiIiqFevnmXc2dmZbt268dlnn7Fp0ya2bt3KgQMHALCzs6Njx458+OGH7N+/n+joaP744497WLPiIXu4d1L7YWjxAuz6ljeufcpip4nsP5fAN3+d4tUHa9/9/UIIUUwCAwNZunQp3bp1Q6fT8c4779xxT7W4DBkyhMmTJ1OnTh2CgoL4/PPPuXr1aoE+bIwYMYK+ffvStGlTOnbsyMqVK1m6dKnlrOu5c+eSnZ1N69atcXFxYd68eTg7O1O9enVWrVrFyZMnadeuHRUqVGD16tWYTCbq1q1bXKtcaLKHezePTADPAAyZyfwv1AGAaeFHOXExWePChBDl2bRp06hQoQJt2rShW7dudOrUiWbNmtm8jlGjRtGvXz+effZZQkJCcHNzo1OnTjg5OeW7jx49evDpp58ydepU6tevz1dffcWcOXN46KGHAPOzaL/++mtCQ0Np1KgR69evZ+XKlXh5eeHp6cnSpUtp3749wcHBfPnll/z888/Ur1+/mNa48HTK1gfji9C5c+eoVq0aZ8+epWrVqsW3oJh94OaLcvPm2e928OexSzSvXoFfXgnBoJcLxIUoSdLS0jh16hQ1a9Ys0B99UTRMJhPBwcH07duXiRMnal1OkbnT71V+s0j2cPPDrzG4+6DT6ZjSuxGuDgb+OX2VH7ZGa12ZEEJo6vTp03z99dccPXqUAwcOMHDgQE6dOsV//vMfrUsrcSRwC6hKzHoW1TJfY/bh2ijOXC6Zp58LIYQt6PV65s6dS8uWLQkNDeXAgQOsX7+e4OBgrUsrcTQN3FmzZtGoUSOMRiNGo5GQkBDWrFmjZUl3dukYLHyGetHf85J/NKmZ2Yxast/mp8gLIURJUa1aNSIiIkhISCAxMZG///67xN7pSWuaBm7VqlWZMmUK//zzD7t27aJ9+/Z0796dQ4cOaVnW7VUKhNCh0PYN/vt0P5zs9Ww9eZmfd5zVujIhhBAlnKaB261bN7p27UpgYCD33XcfkyZNws3NjW3btmlZ1p11fA86jqe6d0VGdAoC4P3Vkfwbn6pxYUIIIUqyEvMdbnZ2NgsWLCAlJYWQkBCty7m9PNeWDbi/Kt38k0hOz+L/lh6QQ8tCCCFuS/MbXxw4cICQkBDS0tJwc3Nj2bJlVncXySs9PZ309HTLeFJSkq3KvFnieQwL+vNJ6hl22L3P5qOwZPe/9GlejJcnCSGEKLU038OtW7cue/fuZfv27QwcOJCwsDAOHz58y7aTJ0/Gw8PDMtwumG3CxQsyU7FLvcQ8v8UATFh5iLjENO1qEkIIUWJpHrgODg7UqVOH5s2bM3nyZBo3bsynn356y7ajR48mISHBMtwumG3CzhF6zgKdgcCLv/Nqpf0kpmUxZvlBObQshBDiJpoH7o1MJpPVYeO8HB0dLZcQGY1G3N01fmqPf1N44E0A3sr6Cl9DAuGHL7Bqf4y2dQkhyqWHHnqIYcOGWcZr1KjB9OnT7/genU5X4AfGF2c/dzJ+/HiaNGlSrMsoTpoG7ujRo9myZQvR0dEcOHCA0aNHs2nTJvr3769lWQXTbgT4NMQu7SrzfBcCindXHOJy8q0/NAghxI26det22wfA//nnn+h0Ovbv31/gfnfu3MnLL798r+VZuV3oxcTE0KVLlyJdVlmjaeDGxcXx7LPPUrduXTp06MDOnTtZt24djzzyiJZlFYydg/nQst6OOpc38WrF3VxJyWD8Sg0PdwshSpUXXniB8PBwzp07d9O8OXPm0KJFC6sHx+dX5cqVcXFxKYoS78rX1xdHR0ebLKu00jRwv/32W6Kjo0lPTycuLo7169eXrrDN4dsQHhwFwFvZ3+Cnv8rKfedZdyhW48KEEKXB448/TuXKlZk7d67V9OTkZBYtWsQLL7zA5cuX6devH1WqVMHFxYWGDRvy888/37HfGw8pHzt2jHbt2uHk5ES9evUIDw+/6T2jRo3ivvvuw8XFhVq1avHOO++QmZkJmB+T995777Fv3z50Oh06nc5S842HlA8cOED79u1xdnbGy8uLl19+meTk3KesDRgwgB49ejB16lT8/Pzw8vJi0KBBlmXlh8lkYsKECVStWhVHR0eaNGnC2rVrLfMzMjIYPHgwfn5+ODk5Ub16dSZPngyAUorx48cTEBCAo6Mj/v7+DB06NN/LLgzNLwsqM9q+AUd+wy5mLz96/0zH2IGMXX6Q1jUr4unioHV1QgiAjJSCv8fgCIbrfyqzsyA7HXR6sHe+c78OrvlehJ2dHc8++yxz585lzJgxlmfJLlq0iOzsbPr160dycjLNmzdn1KhRGI1GfvvtN5555hlq165Nq1at7roMk8lEr1698PHxYfv27SQkJFh935vD3d2duXPn4u/vz4EDB3jppZdwd3dn5MiRPPXUUxw8eJC1a9danlXr4eFxUx8pKSl06tSJkJAQdu7cSVxcHC+++CKDBw+2+lCxceNG/Pz82LhxI8ePH+epp56iSZMmvPTSS/nabp9++ikff/wxX331FU2bNuW7777jiSee4NChQwQGBvLZZ5+xYsUKfvnlFwICAjh79ixnz5rvDLhkyRI++eQTFixYQP369YmNjWXfvn35Wm5hSeAWFYM99JgFsx+kTvxfDPRsxqz4+5m4KpKP+zbWujohBMD7/gV/z5NzoX5P8+sjK2HRAKjeFp77LbfN9IZw7bL1+8YnFGgxzz//PB999BGbN2+2PAd2zpw59O7d23Ip5FtvvWVpP2TIENatW8cvv/ySr8Bdv349R44cYd26dfj7m7fD+++/f9P3rmPHjrW8rlGjBm+99RYLFixg5MiRODs74+bmhp2dHb6+vrdd1vz580lLS+OHH37A1dX8wWPGjBl069aNDz74AB8fHwAqVKjAjBkzMBgMBAUF8dhjj7Fhw4Z8B+7UqVMZNWoUTz/9NAAffPABGzduZPr06cycOZMzZ84QGBhI27Zt0el0VK9e3fLeM2fO4OvrS8eOHbG3tycgICBf2/FelLizlEs1n3rw0GgA3sqeg5/uMkt2n2NjVJzGhQkhSrqgoCDatGnDd999B8Dx48f5888/eeGFFwDz3fgmTpxIw4YNqVixIm5ubqxbt44zZ87kq//IyEiqVatmCVvglnf1W7hwIaGhofj6+uLm5sbYsWPzvYy8y2rcuLElbAFCQ0MxmUxERUVZptWvXx+DwWAZ9/PzIy4uf38vExMTOX/+PKGhoVbTQ0NDiYyMBMyHrffu3UvdunUZOnQov//+u6Xdk08+SWpqKrVq1eKll15i2bJlZGVlFWg9C0r2cItam6FwZBWGf/9hRpX19D73FP+39AC/v9EOdyd7rasTonz7v/MFf48hz4lAQd3Mfehu2FcZduDe6rruhRdeYMiQIcycOZM5c+ZQu3ZtHnzwQQA++ugjPv30U6ZPn07Dhg1xdXVl2LBhZGRkFMmyAbZu3Ur//v1577336NSpEx4eHixYsICPP/64yJaRl7299d9EnU6HyWQqsv6bNWvGqVOnWLNmDevXr6dv37507NiRxYsXU61aNaKioli/fj3h4eG89tprliMMN9ZVVGQPt6gZ7KDHlxAymHrPzaS6lwsxCWlMXnNE68qEEA6uBR8MefZLDHbmaXm/v71dv4XQt29f9Ho98+fP54cffuD555+3fJ8bERFB9+7d+e9//0vjxo2pVasWR48ezXffwcHBnD17lpiY3PsE3PigmL///pvq1aszZswYWrRoQWBgIKdPn7ZeVQcHsrOz77qsffv2kZKS+912REQEer2eunXr5rvmOzEajfj7+xMREWE1PSIiwuouhEajkaeeeoqvv/6ahQsXsmTJEq5cuQKAs7Mz3bp147PPPmPTpk1s3bqVAweK5sPTrUjgFofK90GnSTi7ujOll/lU/vnbz/D38UsaFyaEKMnc3Nx46qmnGD16NDExMQwYMMAyLzAwkPDwcP7++28iIyN55ZVXuHDhQr777tixI/fddx9hYWHs27ePP//8kzFjxli1CQwM5MyZMyxYsIATJ07w2WefsWzZMqs2NWrU4NSpU+zdu5dLly7d8kZF/fv3x8nJibCwMA4ePMjGjRsZMmQIzzzzjOX726IwYsQIPvjgAxYuXEhUVBRvv/02e/fu5fXXXwdg2rRp/Pzzzxw5coSjR4+yaNEifH198fT0ZO7cuXz77bccPHiQkydPMm/ePJydna2+5y1qErjFLKSmJ1PqHkOHiVFL93Mto3i/IxBClG4vvPACV69epVOnTlbft44dO5ZmzZrRqVMnHnroIXx9fenRo0e++9Xr9SxbtozU1FRatWrFiy++yKRJk6zaPPHEE7zxxhsMHjyYJk2a8Pfff/POO+9YtenduzedO3fm4YcfpnLlyre8NMnFxYV169Zx5coVWrZsSZ8+fejQoQMzZswo2Ma4i6FDhzJ8+HDefPNNGjZsyNq1a1mxYgWBgYGA+YzrDz/8kBYtWtCyZUuio6NZvXo1er0eT09Pvv76a0JDQ2nUqBHr169n5cqVeHl5FWmNeelUKb7x77lz56hWrRpnz56latUS+JQepWBebzixgal2LzMj+SEGtKnB+Cfqa12ZEGVWWloap06dombNmjg5OWldjigj7vR7ld8skj3c4qTTwX2dwMGNx1vUAeD7rdHsjL6icWFCCCFsTQK3uLV8CQbvJKjzKzzZvCpKwajF+0nLvPNJB0IIIcoWCdzipteD0fw9zNjH6uHjZs/JSyl8sj7/ZxcKIYQo/SRwbcgj5i82OI+ipi6Gr7ecZN/ZeK1LEkIIYSMSuLaiFER8ilvSSb7z/A6UiRGL95GeJYeWhRCiPJDAtRWdDp74HBzcqZl6iKHO6zh6IZmZG09oXZkQZVJR3rFIiKL4fZJbO9qSZzXo/D6sGMIQ3UJW6hrxxUYdnev7Us/fqHV1QpQJDg4O6PV6zp8/T+XKlXFwcLDcrUmIglJKkZGRwcWLF9Hr9Tg4FP7pbxK4ttb0GTj8K4bj6/nG+C0dE8YwYvE+lg8Kxd4gBxyEuFd6vZ6aNWsSExPD+fOFuHeyELfg4uJCQEAAen3h/05L4NqaTgfdPoMvQqiZfoShTmv45PzjzN5ykkEP19G6OiHKBAcHBwICAsjKyrrrfX+FuBuDwYCdnd09HymRwNWCRxXo8gEsf5Uhul9YrWvEp+v1PFrPh0Afd62rE6JM0Ol02NvbF9uTX4QoKDmGqZXGT8N9XdCrLL5y/xZTdgYjFu8n21Rq77QphBDiDiRwtaLTQbdPwbkCNTKO8YbjKvaejWdOxCmtKxNCCFEMJHC15O4DXacCMFC/lHq6aD5aF8WpSyl3eaMQQojSRgJXaw16Q/AT6FUWs9y+xpSVwagl+zHJoWUhhChTJHC1ptPBY9PAIwBjyPM4ONiz49QVftp+WuvKhBBCFCEJ3JLArTIM+YcK7YcysnM9ACavOcLZK9c0LkwIIURRkcAtKezMdy955v7qtA1wJjsjldFLD6CUHFoWQoiyQAK3hNGf286ctGG86bCUv45f4pddZ7UuSQghRBGQwC1prl3GPvE0T7vswpk0/rcqktiENK2rEkIIcY8kcEuaoMfgic9xHbqV+6r5kpSexZhlcmhZCCFKOwnckqjZsxhcPPmoTyMcDHo2HInj171yE3YhhCjNJHBLsPu83fiy3kGa66IYv/IQF5PStS5JCCFEIUnglmTbZtH+2P/43Hk26deSeHfFQa0rEkIIUUgSuCVZ0/5grIq/KYa37Rey+kAsqw/EaF2VEEKIQpDALcmcPKD75wCEGdYRoj/EuF8PciUlQ+PChBBCFJQEbklXuz00fw6AaY5fk5qcwISVhzQuSgghREFJ4JYGj04EzwD8VBz/Zz+f5XvPs/7wBa2rEkIIUQASuKWBozt0nwlAf8MG2uoPMGb5ARJSMzUuTAghRH5J4JYWNdtBq5cB+Njxa64lXuX93yI1LkoIIUR+SeCWJh3HQ4Wa+KhLjLWfx8JdZ/nz2EWtqxJCCJEPEriliYMr9PgC0PGUYRMP6ffw9pIDJKdnaV2ZEEKIu5DALW2qt4H7XwPgI4dvSYq/yAdrjmhclBBCiLuRwC2N2o8Frzp4OBmorovjx22n2XbystZVCSGEuAMJ3NLIwQWe+gmHoTto0PJBAEYt2U9qRrbGhQkhhLgdCdzSyjsIXCoyumswfh5OnL58jY9/j9K6KiGEELchgVvKGR3t+LbpSf5n9y3fRpxi95mrWpckhBDiFgoVuGfPnuXcuXOW8R07djBs2DBmz55dZIWJfLp8gnrbR/Ffuw200+1j5OL9pGXKoWUhhChpChW4//nPf9i4cSMAsbGxPPLII+zYsYMxY8YwYcKEIi1Q3EWlOvDgKFIfGM0R5+Ycj0vm8z+OaV2VEEKIGxQqcA8ePEirVq0A+OWXX2jQoAF///03P/30E3Pnzi3K+kR+PDQK5w5v817PxgB8ufkkB/9N0LgoIYQQeRUqcDMzM3F0dARg/fr1PPHEEwAEBQUREyPPa9VK5wa+dG/gRV11ircW7SMjy6R1SUIIIa4rVODWr1+fL7/8kj///JPw8HA6d+4MwPnz5/Hy8sp3P5MnT6Zly5a4u7vj7e1Njx49iIqSM20LLeEcH8cP42fHSVyOPcOsTSe0rkgIIcR1hQrcDz74gK+++oqHHnqIfv360bix+VDmihUrLIea82Pz5s0MGjSIbdu2ER4eTmZmJo8++igpKSmFKUu4+WBnZ48HKbxv/y0zNh7lSGyi1lUJIYQAdEopVZg3Zmdnk5iYSIUKFSzToqOjcXFxwdvbu1DFXLx4EW9vbzZv3ky7du3u2v7cuXNUq1aNs2fPUrVq1UIts8y5cAj11YPoTJkMz3iV4/7dWDqwDXYGuQJMCCGKQ36zqFB/hVNTU0lPT7eE7enTp5k+fTpRUVGFDluAhATziT4VK1a85fz09HQSExMtQ1JSUqGXVWb51Ef30NsAjHf4gQvnTvHNX6e0rUkIIUThArd79+788MMPAMTHx9O6dWs+/vhjevTowaxZswpViMlkYtiwYYSGhtKgQYNbtpk8eTIeHh6WoV69eoVaVpkXOgz8m2HkGlPsv2ZaeBQnLiZrXZUQQpRrhQrc3bt388ADDwCwePFifHx8OH36ND/88AOfffZZoQoZNGgQBw8eZMGCBbdtM3r0aBISEizD4cOHC7WsMs9gBz2/RBkcediwjx7qD0Yu3k+2qVDfHgghhCgChQrca9eu4e7uDsDvv/9Or1690Ov13H///Zw+fbrA/Q0ePJhVq1axcePGOx7/dnR0xGg0WoacGsQtVK6Lrv1YAMbZzSPm9DG+/zta25qEEKIcK1Tg1qlTh+XLl3P27FnWrVvHo48+CkBcXBxGozHf/SilGDx4MMuWLeOPP/6gZs2ahSlH3E7IIKjaCjddKh/Yz+bDdZGcvixngAshhBYKFbjjxo3jrbfeokaNGrRq1YqQkBDAvLfbtGnTfPczaNAg5s2bx/z583F3dyc2NpbY2FhSU1MLU5a4kd4APWah7Jx5wHCQ3qZw3l5yAJMcWhZCCJsr9GVBsbGxxMTE0LhxY/R6c27v2LEDo9FIUFBQ/hau091y+pw5cxgwYMBd3y+XBeXTtlmw9m1SlCOdMj5gYI/29G9dXeuqhBCiTMhvFtkVdgG+vr74+vpanhpUtWrVAt30AsyHlIUNtHoFIlfiejqC1wwreH+1Pw/V9aaKp7PWlQkhRLlRqEPKJpOJCRMm4OHhQfXq1alevTqenp5MnDgRk0nu31vi6PXQfSamdiP51X8YyelZ/N/SA/KBRwghbKhQgTtmzBhmzJjBlClT2LNnD3v27OH999/n888/55133inqGkVRqFgTffsxTOrTHAc7PZuPXmTJ7n+1rkoIIcqNQh1S/v777/nmm28sTwkCaNSoEVWqVOG1115j0qRJRVagKFp1vN14s31NTmz4hv+t1NMusBLeRietyxJCiDKvUIF75cqVW54YFRQUxJUrV+65KFGMlOKl02+it/8L98xrjFlemdnPNL/tCWxCCCGKRqEOKTdu3JgZM2bcNH3GjBk0atTonosSxUinQ9/oSbIdPbik8yL88AVW7pdnGAshRHEr1B7uhx9+yGOPPcb69est1+Bu3bqVs2fPsnr16iItUBSDZmEYgh6n5tYrsP4Y41ccIrS2F15ujlpXJoQQZVah9nAffPBBjh49Ss+ePYmPjyc+Pp5evXpx6NAhfvzxx6KuURQ1nQ5cK/HaQ3UI8nUnMeUa7644pHVVQghRphX6xhe3sm/fPpo1a0Z2dnZRdXlHcuOLexcdsQj7399mQMZI3uzfnc4NfLUuSQghSpVifR6uKCOUosbpxVTRXeJj+1mMX76X+GsZWlclhBBlkgRueabTwePTUU6eNNKfok/qYiauitS6KiGEKJMkcMs7ox+6rh8BMNRuKYf3RLAxKk7jooQQouwp0FnKvXr1uuP8+Pj4e6lFaKXhk3D4VxyOrOJj+y95ZUktfhveAaOTvdaVCSFEmVGgwPXw8Ljr/GefffaeChIa0Ong8U9Qp/+mXupp+lxbyOTVVZjcq6HWlQkhRJlRoMCdM2dOcdUhtObmje6xj2HxcwwyLKfHzmZENPIjtE4lrSsTQogyQb7DFbka9IL6PbHTmfjY/kveWfIPKelZWlclhBBlggSusNb1Y0wulamrP0efpHl8tC5K64qEEKJMkMAV1ly90Hf7BIBXDCvZt209O6PlgRRCCHGvJHDFzYK7QcMnMegUU+1mMXbRLtIybXP3MCGEKKskcMWtdfmQ7Ip1mG/fk6jLGXyy/qjWFQkhRKkmgStuzaUihsE7uL/3MEDH11tOsvdsvMZFCSFE6SWBK25Pb+CRej50b+KPm0rhnUU7SM+SQ8tCCFEYErjiriY2usLvTm/T88q3zPzjuNblCCFEqSSBK+7K6KDw5TIP6vfx3abDHDqfoHVJQghR6kjgirur0xHV+1s+rf0NySYHRi7eT2a2SeuqhBCiVJHAFfmia9iHsb1a4Oliz6HziczeclLrkoQQolSRwBX55u3uxLuPB/GcYQ07Nizl2IUkrUsSQohSQwJXFEiPtF951/5HJhm+Ytwv28g2Ka1LEkKIUkECVxSIrvlzZBkDqKq7xBMXZvLdX6e0LkkIIUoFCVxRMI5u2PWaBUA/u41sD1/IqUspGhclhBAlnwSuKLgabVGtXwVgon427/3yNyY5tCyEEHckgSsKRdfhXTI9a+Gnu8LjMZ8xb/tprUsSQogSTQJXFI6DC/a9vkSho49hC9vW/MTZK9e0rkoIIUosCVxReAGtIWQwAON1s5m0KAKl5NCyEELcigSuuCe69mPJqFAHb108nc9N45ddZ7UuSQghSiQJXHFv7J1w6D0bE3p6GP5m66q5xCakaV2VEEKUOBK44t5VbQ5t3wBgLF8zefGfcmhZCCFuIIErioT+oVGkV6xLOg4cP36U5Xv/1bokIYQoUSRwRdGwc8TxvwtYFbqYQ6oG41ccJi5JDi0LIUQOCVxRdCrW4vkOjannZyQhNZN3fz2kdUVCCFFiSOCKImVv0PNRn4b8124Dj0SNY/X+81qXJIQQJYIErihy9Z0u85799/Qy/MW65T9wJSVD65KEEEJzErii6HnVRrV/hy+cXmTFtfpMWCmHloUQQgJXFAu7B4bRpv876HR6lu89z/rDF7QuSQghNCWBK4pNk2qevPRALZxJ4+eli0lIzdS6JCGE0IwErihWb7RyIdx5DJ9kTmTGso1alyOEEJqRwBXFyqliVTwr+WLUpfJA5AS2RMVpXZIQQmhCAlcUL70Bt6e+JlPnQDvDAbYumkZyepbWVQkhhM1pGrhbtmyhW7du+Pv7o9PpWL58uZbliOJSKRBT+3EADMqcw1fL5dCyEKL80TRwU1JSaNy4MTNnztSyDGEDjqGvkVC5BW66NEIOjmPbiYtalySEEDalaeB26dKF//3vf/Ts2VPLMoQt6A14PD2bDJ0TbQyH2brwQ1IzsrWuSgghbEa+wxW241Wb7I7jAXgl/Xu+W7Fe23qEEMKGSlXgpqenk5iYaBmSkpK0LkkUkHPIK1z1vh8XXTqt9r/DP9GXtS5JCCFsolQF7uTJk/Hw8LAM9erV07okUVB6PRX6zSZN70xLfRTbf/4faZlyaFkIUfaVqsAdPXo0CQkJluHw4cNalyQKo0J1TB0nAvB82o/MWyWHloUQZV+pClxHR0eMRqNlcHd317okUUguIS9yyTsUJ10mzrtnc+BcgtYlCSFEsdI0cJOTk9m7dy979+4F4NSpU+zdu5czZ85oWZawBZ2OSv1ns6zSK4zLDGPE4n0c/DdBDi8LIcosOy0XvmvXLh5++GHL+PDhwwEICwtj7ty5GlUlbMajKu0GTMTjky0ciU1i56yXiNPHsdztaUxVWxHsZ6Sp6xWCMw/iWbEyOidPcPaEnJ8ObqDTabsOQgiRT5oG7kMPPYRSSssShMa83ByZ/UxzpoUf5f5/jxLMKX6If4RNV2JYtT+GJw2baGM/+5bvVToDOieP3BDOee1SCR6bmtvw7A7ISAGfBuBW2QZrJYQQN9M0cIUAaFGjIvNfuh91bCoJcad5yaUVrRNcOBKbiOMZPzYnN8adFDxIwagz/3TQZaNT2ZB6xTzkoZwrossbuBsmQPSf0PtbaNjHPO3Ib7Ds1et7yx7Wge3kmbsXnfenkwdUCpS9aiFEoUjgihJDF9gRj0AIxTyYNSU963VOxKWwLzaRyJhEjsQkcjLmMlkpV/DQpWAkBQ9dTiBfw8GkZ9cXEQT7GQnyM9LZ3o+KlYMxuPnkLiz1KqQnmof8nq+lM8C4PNcNr3wdzmyHh/8P6j1hnnb5BOz/xTq8nT3N4zmv7V0ktIUohyRwRYnnaGegnr+Rev5Gq+kXk9KJik3iSGwikTFJRMYkcjwumYwME5yJZ/eZeADeoQfQg6q/ZBLku4tgP3caVL6fev/ZjL9TBob0REiLN4dwWjykJUBq/M2vdXrroLx8Ai5GQlZ67rS4w7B5yp1XSG9/cyA/PR/sHM3zj6+HxPNQ7X6ofJ95WlYGZF4DRyPoS9XFBUKI6yRwRalV2d2Ryu6OtA2sZJmWmW3i1KUU855wbBJHrv+MSUjj3NVUzl1NZX3kBUt7J3s9dX3cCfK9jyA/d4KqGAn2c8fTxeHuBTz2sTkYvYNzp3lUhRbPWwd1arx5PC0eTFlgyoRrl8wDmPecDXmW989ciFwJXafmBu7Z7fD944Du+t7yHfag8x4er9EO7K73rZTsWQuhIQlcUabYG/Tc5+POfT7udM8z/WpKhjmAYxM5EmP+GXUhibRME/vOJbDvhuuA/TycCPJ1J8jPSJCvO8F+RmpWcsXekGfvsnJd85CXf1PzcCtKmU/eunHPOeOadRD6NzPv0XrVyZ2WnpjTyfX3x0P86btvkNHncgN3xRA4tAzavwP3v3p9w0TDpilgsAe9nXnQGUBvyB23DPrc183CwNHN3Me5XXDpqPmkNL9G1+tNhtMR5n50N/altx7Puzw3n9x6s9LNH1AMDub6hCjlJHBFuVDB1YGQ2l6E1PayTMs2KU5fTuFIrPlwdOT1ID53NZWYhDRiEtLYGJX7GEEHg55AHzeCfM17wUG+RoL83Knk5pi/InQ6c0g5upn3hG/ngeE3Twt6DMbG3fpQd9496Lzz0xLMl07lSIuHjGRzuOWIPwv7fs5f/Xk16JMbuPsXwo7Z0G5kbuAmnof5fQve76sR4NvA/DriU9g4CZoPgG6fmqdduwIfB+UJ7Bs/HNwY8NfnP/YxVGlu7uPoOnO9AfdDuxG5y176svlrg7z9Wfq6zYeGul1zP3RdjYaTm8HdD+57NLffY+GQnXlDPzf0l/OhwmBvfu3kCQ4u5vfnXMkhRydKPQlcUW4Z9DpqVXajVmU3ujb0s0xPTMvkaGwSkdeD+EhMIlGxSaRkZHPofCKHzida9VPJzZFgP/NecJCvOYhre7viaGe4cZH3xs4R3H3MQ2E8MQM6vgfOFXKneQaYp5kywWS6fsg7Z8gGlX3zNFM22Dvn9lHpPqjdAbxq504z2Jv31C3vycrT14195lmuPs+fJFOW+afVtGzITjcPBZFxLfd1/Bnz9+QOrrnTlDJ/cCgoz+q5gXt+D6wcCtVDrQN32au5Xx/kV5cPofUr5ten/4a5j4F3PXjt79w2P/Qwh3xOSBvszecHGBzAkBPiDnkC3cH8wS2oq/n9yRfNHzwc3SF0aG6/R9dByqWbPwTk/NTfOC3nnITrv1dK5f5byocEKxK4QtzA6GRPixoVaVGjomWayaQ4dzWVyDyHpI/EJhF9OYVLyen8eSydP4/l/lG10+uoXdnN/L3w9T3hen5GvN0d0Wn1R8jZ0zzkVaE6tB12b/22esk85FWxJry88d76bTcSQoeZ9zpzuFSEYQevB/gtPiDkDfO8Ae9TP7ePWg9Bjy/Bo0ruNKWg0/t53n+bfm78oFChem4fbr7X93iDrNfDv6n5hLxbffjIvt5XdoZ5L9iUaX6d9xC6KRO4xf0KEs7C1VMF26aeAbmBmxIHWz4E18rWgfvXdDjz9y3fflstX8q99j3lIkwNNL9+Nz43dFcMgeMbbvhgcJdAr9Yq93dLKVg/3jy97bDcD0zREeavNHI+VNzyw4a9df+O7mD0w9YkcIXIB71eR4CXCwFeLnSq72uZnpKexdELSZYTtCKv/0xMyyLqQhJRF5L4lfOW9hVc7C0BHHz9530+7jjZF/HecFlgsDMPeekN4Fnt3vqtFGgerPrVQ8ige+u3eoh5uNF/F99bvwFt4K1juYeWc/T9wXxOQHZGbmBnZ94c3nmnB+Spz8nTHJQ5h65zVG1hDrPsjDwfBjLMHw7yLitv//ZOue/PzjT/1Ntb7+GmXILEfwu48io3cE1ZEDHd/DrkNeB64O5fCLu/L1i3NR+EsBUFrOXe6VQpvtXTuXPnqFatGmfPnqVq1Tt8JyaEDSmlOJ+QZjlDOueM6ZMXkzHd4n+bXgc1K7kS5Gck2Dd3j7iKp7N2e8NCFJbJBOkJ5uB1886dfjXavKefN7jvFuhedeC+Tub3Z2dC+Lvm6Y9MzA35HV/DiY3X+7vhQ8ftPnjUaAtP/1Rkq5zfLJLAFcJG0jKzOR6XbHWCVmRMIlevZd6yvbuTnWUvOCeE6/q44+ooB6aEKEnym0XyP1cIG3GyN9CgigcNqnhYpimluJiUbjkUnbNHfDwumaS0LHZEX2FHtPWtK6t7uVguVco5Y7paBRf0etkbFqIkk8AVQkM6nQ5voxPeRicevC/3wQoZWSZOXEy2XDecc8b0xaR0Tl++xunL11h3KPcGHi4OBupePxydc8Z0XV93jE5y/aoQJYUErhAlkIOdnmA/I8F+RshzH41LyebbWVrupBWbyNELyVzLyGbPmXj2XL+dZY4qns5W1wwH+Zpv4GGQvWEhbE4CV4hSpJKbI5XqOBJaJ/d2llnZJqIvp3A4Jvew9JGYRM4npPFvfCr/xqeyPjLO0t7RTn99b9gcwPf5uOPiaMBOr8Og12Fv0GPQ6yzjdnr99enW43Z6nRzGFqIAJHCFKOXsDHrqeLtTx9udJxr7W6YnXMu0XC8cef2SpaOxSaRmZrP/XAL7b7idZWHodVgFsMFg/mmZZglpHQa9Pk9o54a3neHmcLezvFefZ565f/tbLO/mvvWWPvKO5/Z9w4eK29Z984cNvQ45e1wUigSuEGWUh4s9rWt50bqW9e0sz1y5ZnXN8ImLyWRkm8jOVmSZFNkm88+sbJPV+K2YFGRkmyDbVmtVMuSG+Y1HA8xhnjNuyPPBwf6Gcbvr7d0c7TA62+PuZIfRyR6jc85Pe8u4u5N5vtW9vEWpI4ErRDli0OuoWcmVmpVc6dIw/3faUUphUuanMeUEsPnn9XFLWJuuh3Xu/NzXtw/zvO/LO56drci8YTzvsm/u25Snjzz13apvy7Q863S9TZbJRGb27a+YzLJ8CDEVwb9K/rk4GKxC2d3JziqYc4I6N7ztMTrlBnqR325UFIgErhDirnQ6HQYdGPTl6w+2qUBhfsOHkevT7vRBIdNkIiU9i8TULBLTMklMzSQxLYuktEyraSkZ5kMI1zKyuZaRTWziXQq/DUc7vVUI3xzQ1kHt7mSPR55pjnZ6OZx+DyRwhRDiNvR6HQ6WE8O0+7CRlW0iKS2LpLS8wZwnlNOybp6Wmmlpn5RmfhBEepaJi0npXEwq4MMfrnMw6Auwd219aNzdyQ4XB0O5DmwJXCGEKOHsDHoquDpQwdWhUO/PNimS03NDOckS0DcHtdXe9fXXSWmZlu/rLyVncCk5o1B1GPS6G/au7XB3vHnP2hzcefbEr093dbAr1WfGS+AKIUQZZ9Dr8HC2x8O5cDdCUUqRkpFtHc43hbd1UN8Y6jmH1K9ey7zt7UzvRq/DcpLZ7faibz5kfr2Nkz1uTnaaXoMugSuEEOKOdDrz2dRujnb443z3N9xAKUVqZrZlb/nGQ9+JlkPlN3yXnSewM7JNmBTX22YBqYVaF/c8Z4VX8XTm2wEtC9VPYUjgCiGEKFY6nQ4XBztcHOzw9XC6+xtuIS0z+4ZD33f+3vrGQ+ZpmeYzypPSs0hKN3+nnZKRVWTrmB8SuEIIIUo8J3sDTvYGvN0L9/6MLNP1vevcELb1s/IkcIUQQpR5DnZ6vNwc8XJz1KwGuW2JEEIIYQMSuEIIIYQNSOAKIYQQNiCBK4QQQtiABK4QQghhA6X6LGWTyXxdVUxMjMaVCCGEKK9yMignk26nVAfuhQsXAGjVqpXGlQghhCjvLly4QEBAwG3n65Sy9aW/RScrK4s9e/bg4+ODXn9vR8eTkpKoV68ehw8fxt29kFdWC1EKye++KI+K8vfeZDJx4cIFmjZtip3d7fdjS3XgFqXExEQ8PDxISEjAaDRqXY4QNiO/+6I80uL3Xk6aEkIIIWxAAlcIIYSwAQnc6xwdHXn33XdxdNTuPptCaEF+90V5pMXvvXyHK4QQQtiA7OEKIYQQNiCBK4QQQtiABK4QQghhAxK4182cOZMaNWrg5ORE69at2bFjh9YlCVGstmzZQrdu3fD390en07F8+XKtSxKiWE2ePJmWLVvi7u6Ot7c3PXr0ICoqymbLl8AFFi5cyPDhw3n33XfZvXs3jRs3plOnTsTFxWldmhDFJiUlhcaNGzNz5kytSxHCJjZv3sygQYPYtm0b4eHhZGZm8uijj5KSkmKT5ctZykDr1q1p2bIlM2bMAMy36apWrRpDhgzh7bff1rg6IYqfTqdj2bJl9OjRQ+tShLCZixcv4u3tzebNm2nXrl2xL6/c7+FmZGTwzz//0LFjR8s0vV5Px44d2bp1q4aVCSGEKE4JCQkAVKxY0SbLK/eBe+nSJbKzs/Hx8bGa7uPjQ2xsrEZVCSGEKE4mk4lhw4YRGhpKgwYNbLLMUv14PiGEEKIwBg0axMGDB/nrr79stsxyH7iVKlXCYDBYnq2b48KFC/j6+mpUlRBCiOIyePBgVq1axZYtW6hatarNllvuDyk7ODjQvHlzNmzYYJlmMpnYsGEDISEhGlYmhBCiKCmlGDx4MMuWLeOPP/6gZs2aNl1+ud/DBRg+fDhhYWG0aNGCVq1aMX36dFJSUnjuuee0Lk2IYpOcnMzx48ct46dOnWLv3r1UrFiRgIAADSsTongMGjSI+fPn8+uvv+Lu7m45T8fDwwNnZ+diX75cFnTdjBkz+Oijj4iNjaVJkyZ89tlntG7dWuuyhCg2mzZt4uGHH75pelhYGHPnzrV9QUIUM51Od8vpc+bMYcCAAcW/fAlcIYQQoviV++9whRBCCFuQwBVCCCFsQAJXCCGEsAEJXCGEEMIGJHCFEEIIG5DAFUIIIWxAAlcIIYSwAQlcIYQQwgYkcIUQ+aLT6Vi+fLnWZQhRakngClEKDBgwAJ1Od9PQuXNnrUsTQuSTPLxAiFKic+fOzJkzx2qao6OjRtUIIQpK9nCFKCUcHR3x9fW1GipUqACYD/fOmjWLLl264OzsTK1atVi8eLHV+w8cOED79u1xdnbGy8uLl19+meTkZKs23333HfXr18fR0RE/Pz8GDx5sNf/SpUv07NkTFxcXAgMDWbFihWXe1atX6d+/P5UrV8bZ2ZnAwMCbPiAIUZ5J4ApRRrzzzjv07t2bffv20b9/f55++mkiIyMBSElJoVOnTlSoUIGdO3eyaNEi1q9fbxWos2bNYtCgQbz88sscOHCAFStWUKdOHatlvPfee/Tt25f9+/fTtWtX+vfvz5UrVyzLP3z4MGvWrCEyMpJZs2ZRqVIl220AIUo6JYQo8cLCwpTBYFCurq5Ww6RJk5RSSgHq1VdftXpP69at1cCBA5VSSs2ePVtVqFBBJScnW+b/9ttvSq/Xq9jYWKWUUv7+/mrMmDG3rQFQY8eOtYwnJycrQK1Zs0YppVS3bt3Uc889VzQrLEQZJN/hClFKPPzww8yaNctqWsWKFS2vQ0JCrOaFhISwd+9eACIjI2ncuDGurq6W+aGhoZhMJqKiotDpdJw/f54OHTrcsYZGjRpZXru6umI0GomLiwNg4MCB9O7dm927d/Poo4/So0cP2rRpU6h1FaIsksAVopRwdXW96RBvUXF2ds5XO3t7e6txnU6HyWQCoEuXLpw+fZrVq1cTHh5Ohw4dGDRoEFOnTi3yeoUojeQ7XCHKiG3btt00HhwcDEBwcDD79u0jJSXFMj8iIgK9Xk/dunVxd3enRo0abNiw4Z5qqFy5MmFhYcybN4/p06cze/bse+pPiLJE9nCFKCXS09OJjY21mmZnZ2c5MWnRokW0aNGCtm3b8tNPP7Fjxw6+/fZbAPr378+7775LWFgY48eP5+LFiwwZMoRnnnkGHx8fAMaPH8+rr76Kt7c3Xbp0ISkpiYiICIYMGZKv+saNG0fz5s2pX78+6enprFq1yhL4QggJXCFKjbVr1+Ln52c1rW7duhw5cgQwn0G8YMECXnvtNfz8/Pj555+pV68eAC4uLqxbt47XX3+dli1b4uLiQu/evZk2bZqlr7CwMNLS0vjkk0946623qFSpEn369Ml3fQ4ODowePZro6GicnZ154IEHWLBgQRGsuRBlg04ppbQuQghxb3Q6HcuWLaNHjx5alyKEuA35DlcIIYSwAQlcIYQQwgbkO1whygD5ZkiIkk/2cIUQQggbkMAVQgghbEACVwghhLABCVwhhBDCBiRwhRBCCBuQwBVCCCFsQAJXCCGEsAEJXCGEEMIGJHCFEEIIG/h/45dQjDMEPvAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_process import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe from the plot\n",
    "- The training loss decreasing steeply over the first epoch indicating that the model has learned the representations in the data very well.\n",
    "- However as we approach the end of the second epoch , the model's loss tends to stabilize indicating that it is converging to a stable solution\n",
    "\n",
    "\n",
    "The alpaca dataset : https://huggingface.co/datasets/tatsu-lab/alpaca\n",
    "- Consists of 52k instruction-output pairs\n",
    "\n",
    "\n",
    "How to possibly deal with memory problems while training on large datasets like these\n",
    "- Reduce batch size\n",
    "- Reduce the `allowed_max_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes as task. Write a response that approximately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes as task. Write a response that approximately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> A type of thunderstorm is the Ursa Major.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes as task. Write a response that approximately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is George Orwell.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#inspecting the responses manually\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256,\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Well the GPT-2 small model clearly doesn't to be working as good as we expect. Understandable considering it size , probably do we need to overtrain it to work to some extent , let me try it out again.\n",
    "- Weird how compared to running it for 2 epochs , training it over 5 epochs does seem quite fine compared to the previous one.\n",
    "    - Can we make the model really improve its capability for this size ? \n",
    "        - Need to read more about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:21<00:00,  5.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "        )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "        )\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a cheetah.'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-small124M-sft.pth\n"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "import re\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model and use it\n",
    "from previous_process import GPTModel\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "model.load_state_dict(torch.load(\"gpt2-small124M-sft.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now use Ollama to automate the evaluation process of the model. Would be using a powerful model Qwen3 , however would be using the quantized smaller version of the model due to memory limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\n",
    "    \"Ollama not running. Launch ollama before proceeding.\"\n",
    "    )\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will serve the model using Ollama locally via an API to which we can send requests and interact with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def query_model(prompt,model=\"qwen3:0.6b\",url=\"http://localhost:11434/api/chat\"):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\" : {\n",
    "            \"seed\" : 42,\n",
    "            \"temperature\" : 0,\n",
    "            \"num_ctx\" : 2048\n",
    "        }\n",
    "    }\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(url,data=payload,method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "    response_data = \"\"\n",
    "\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    \n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking about the capital of France. I know that France is a country in Europe, and I remember that the capital is Paris. But wait, let me make sure. I've heard that sometimes there are other cities with similar names, but I don't think Paris is the capital. Maybe I should double-check. Oh, right! Paris is indeed the capital. Let me think if there's any other possibility. No, I don't think so. The capital is Paris. So the answer should be Paris.\n",
      "</think>\n",
      "\n",
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "model = \"qwen3:0.6b\"\n",
    "result = query_model(\"what is the capital of France?\", model=model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this as an LLM-as-a-judge concept where we prompt the model to evaluate the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Rewrite the sentence using a simile.',\n",
       " 'input': 'The car is very fast.',\n",
       " 'output': 'The car is as fast as lightning.',\n",
       " 'model_response': 'The car is as fast as a cheetah.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"instruction-data-with-response.json\", \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "\n",
      "Score:\n",
      ">> <think>\n",
      "Okay, let's see. The user wants me to rewrite the sentence \"The car is very fast.\" using a simile and then score the model response given by the trained model, which is \"The car is as fast as a cheetah.\" on a scale from 0 to 100.\n",
      "\n",
      "First, I need to check if the model's response uses a simile. The original instruction says to rewrite the sentence using a simile. The input sentence is \"The car is very fast.\" So the model's response is \"The car is as fast as a cheetah.\" which is a simile comparing the car to a cheetah. That's correct.\n",
      "\n",
      "Now, the score is from 0 to 100. The user wants me to approximate the model response's score. Since the model's response is a simile, and the instruction is to complete the request, I need to evaluate how well the model's response meets the criteria. The correct simile is used, so the score should be high. Let me think of a possible score. If the model correctly used a simile, the score is likely around 80-90. Let me check if there's any other factors. The original instruction says \"approximate\" so maybe the model's response is close to the correct one. So the score would be, say, 85. But I need to make sure I'm not making any mistakes here. The key points are that the model used a simile and completed the request correctly. So the score should be high, maybe 85.\n",
      "</think>\n",
      "\n",
      "The model's response correctly uses a simile (\"The car is as fast as a cheetah\") and completes the request. A score of **85** is appropriate, as the response accurately fulfills the instruction.  \n",
      "\n",
      "**Score:** 85.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> A type of thunderstorm is the Ursa Major.\n",
      "\n",
      "Score:\n",
      ">> <think>\n",
      "Okay, let's see. The user is asking what type of cloud is typically associated with thunderstorms, and the correct answer is cumulonimbus. The model response given is \"A type of thunderstorm is the Ursa Major.\" I need to score this response on a scale from 0 to 100.\n",
      "\n",
      "First, I should check if the response correctly identifies the cloud type. The correct answer is cumulonimbus, but the model response is referring to the Ursa Major. That's a mistake. So the model response is incorrect.\n",
      "\n",
      "Now, how to score it? If the response is completely wrong, it's probably a 0. But maybe the user expects a 100? Wait, no, the instruction says to approximate complete the request. Wait, the user provided the instruction and the model response. The model response is incorrect. So the correct answer is cumulonimbus, but the model's response is wrong. Therefore, the score should be 0 because it's incorrect.\n",
      "\n",
      "Wait, but maybe the user is testing if the model can recognize the correct cloud type. If the model's response is wrong, then the score is 0. So the correct answer is cumulonimbus, and the model's response is wrong, so the score is 0.\n",
      "</think>\n",
      "\n",
      "0.0  \n",
      "The model response incorrectly identifies the cloud type as \"Ursa Major\" instead of \"cumulonimbus.\" A correct response would be \"The type of cloud typically associated with thunderstorms is cumulonimbus.\"\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is George Orwell.\n",
      "\n",
      "Score:\n",
      ">> <think>\n",
      "Okay, let's see. The user wants me to check if the model's response about the author of 'Pride and Prejudice' is correct. The instruction says to name the author correctly as Jane Austen. The model's response was George Orwell. Wait, that's a big mistake. I need to correct that.\n",
      "\n",
      "First, I should confirm that George Orwell is the author of 'Animal Farm'. Oh right, 'Pride and Prejudice' is by Jane Austen. So the model's answer is wrong. The correct answer is Jane Austen. The model's response is incorrect. So I need to give a score of 0 because it's wrong. The scale is 0-100, with 100 being the best. So the model's response is incorrect, so the score should be 0.\n",
      "</think>\n",
      "\n",
      "The model response is incorrect. The correct author of \"Pride and Prejudice\" is Jane Austen. The score should be 0.  \n",
      "\n",
      "**Score:** 0\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "    f\"You are a helpful assitant who can score model responses\"\n",
    "    f\"Given the input `{format_input(entry)} and correct output `{entry['output']}`, \"\n",
    "    f\"score the model response given by the trained model`{entry['model_response']}`\"\n",
    "    f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on experimenting , got to come with a simple prompt that does the job of inspecting the responses and providing a score based on the quality of the response. We can now evaluate the responses of the trained model , however changes needs to be done to the prompt slightly\n",
    "Since Qwen3 is a reasoning model\n",
    "- we need to add the no_think to the end of the prompt to only return the responses\n",
    "- Also create another function to make sure i would be able to extract only the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_score(text):\n",
    "    matches = re.findall(r\"\\b([0-9]{1,3})\\b\", text)\n",
    "    for match in matches:\n",
    "        num = int(match)\n",
    "        if 0 <= num <= 100:\n",
    "            return num\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_score_and_trace(response: str):\n",
    "    think_match = re.search(r\"<think>(.*?)</think>\", response, re.DOTALL)\n",
    "    trace = think_match.group(1).strip() if think_match else None\n",
    "    matches = re.findall(r\"\\d{1,3}\", response)\n",
    "    if matches:\n",
    "        score = int(matches[-1])\n",
    "        if 0 <= score <= 100:\n",
    "            return score, trace\n",
    "    return None, trace\n",
    "\n",
    "\n",
    "def generate_model_scores(json_data,json_key,model=\"qwen3:0.6b\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data,desc='Scoring responses'):\n",
    "        prompt = (\n",
    "            f\"You are a helpful assitant who can score model responses\"\n",
    "            f\"Given the input `{format_input(entry)} and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response given by the trained model`{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Please return only the score as an integer without any additional text. /no_think\"\n",
    "            )\n",
    "        raw_score = query_model(prompt, model)\n",
    "        score = extract_score(raw_score)\n",
    "        if score is not None:\n",
    "            scores.append(score)\n",
    "        else:\n",
    "            print(f\"Could not extract valid score: {raw_score}\")\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring responses: 100%|██████████| 110/110 [00:10<00:00, 10.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 86.44\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the final average score , chances are you might feel the model is performing quite well. However here is the catch, i got to previosuly glance through the results generated after training the model and saw some aweful responses.(which is completely understandable).\n",
    "Hence it would be better if we are able to store the \"reasoning\" aspect of this small model used, this is helpful\n",
    "- Enable us to understand the capability of the model used as judge\n",
    "- The prompt can be further improved to make the model judge better.\n",
    "\n",
    "\n",
    "First let me start with improving the prompt a bit and check the results, before i can store the reasoning tokens.(We are severely memory constrained as well hence trying to make the maximum use, so can't download more bigger models which can take upto 16gb memory for inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_judge = \"\"\"\n",
    "You are a helpful and precise assistant that evaluates the quality of model-generated responses.\n",
    "\n",
    "You will be given:\n",
    "- An input\n",
    "- The correct (reference) output\n",
    "- A response generated by a trained model\n",
    "\n",
    "Your task is to assign a score from 0 to 100 based on how closely the model's response matches the reference output in terms of correctness, relevance, and completeness.\n",
    "\n",
    "A score of:\n",
    "- 100 means the response is correct and perfect to the reference output.\n",
    "- 0 means it is completely incorrect or irrelevant to the expected output.\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Reference Output:\n",
    "{reference_output}\n",
    "\n",
    "### Model Response:\n",
    "{model_response}\n",
    "\n",
    "Return only the score as an integer (no explanation, no formatting, no additional text).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring responses: 100%|██████████| 110/110 [01:59<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 25.41\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_model_scores_v2(json_data, json_key, model=\"qwen3:0.6b\"):\n",
    "    evaluations = []\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc='Scoring responses'):\n",
    "        formatted_prompt = prompt_judge.format(\n",
    "            input_text=format_input(entry),\n",
    "            reference_output=entry['output'],\n",
    "            model_response=entry[json_key]\n",
    "        )\n",
    "        raw_score = query_model(formatted_prompt, model)\n",
    "        score,trace = extract_score_and_trace(raw_score)\n",
    "        eval_entry = {\n",
    "            \"instruction\": entry[\"instruction\"],\n",
    "            \"input\": entry[\"input\"],\n",
    "            \"output\": entry['output'],\n",
    "            \"model_response\": entry[json_key],\n",
    "            \"trace\": trace if trace else \"N/A\",\n",
    "            \"score\": score if score is not None else \"INVALID\"\n",
    "        }\n",
    "\n",
    "        evaluations.append(eval_entry)\n",
    "        if score is not None:\n",
    "            scores.append(score)\n",
    "        else:\n",
    "            print(f\"Could not extract valid score: {raw_score}\")\n",
    "    \n",
    "    with open(\"inspect_eval.json\", \"w\") as f:\n",
    "        json.dump(evaluations, f, indent=2)\n",
    "        \n",
    "    return scores\n",
    "\n",
    "scores = generate_model_scores_v2(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the improved prompt , we can see eval process to be more strict and better.\n",
    "This entire pipeline can be improved and more experiments can be conducted. Changes that can be made :\n",
    "- Finetuning the hyper-parameters\n",
    "- Increasing the base-model size and fine-tuning it\n",
    "- Updating the judge model (bigger size more capabilities present(not always true)) and changing the prompt (continous refinement) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
