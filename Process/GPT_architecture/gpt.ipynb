{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "GPT consists of 124M parameters. Parameters refers to the number of trainable weights\n",
    "\n",
    "we will now have a configuration specifing the model architecture. The configuration will be a dictionary with the following keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_config = {\n",
    "    \"vocab_size\":50257,\n",
    "    \"context_length\":1024,\n",
    "    \"emb_dim\":768,\n",
    "    \"n_heads\":12,\n",
    "    \"n_layers\":12,\n",
    "    \"drop_rate\":0.1,\n",
    "    \"qkv_bias\":False # query key value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone of GPT architecture:\n",
    "\n",
    "The idea is to assemble the GPT architecture first by creating a backbone which helps us understand the overall structure of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module): #placeholder for the transformer block\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self,normalized_shape,eps=1e-5): #placeholder for layernorm\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x\n",
    "\n",
    "class DummyGPT(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super(DummyGPT,self).__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.tr_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg)\n",
    "              for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False)\n",
    "    \n",
    "    def forward(self,in_idx):\n",
    "        batch_size , seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len,device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.tr_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview of the process\n",
    "\n",
    "Given an input to the model\n",
    "- it is tokenized\n",
    "- then embedded and fed to the GPT model\n",
    "The output token dimensions matches the input token dimensions and each of the output token is of 50,257 dimensions as specified in the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[16833,  1110,   318,   257,   922],\n",
      "        [ 1169,  6766, 32481,   290,   318]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"every day is a good\"\n",
    "txt2 = \"the sky shines and is\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch,dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size:torch.Size([2, 5, 50257])\n",
      "tensor([[[ 2.2379e-01,  1.9160e+00,  1.4026e+00,  ...,  2.5329e-01,\n",
      "           8.0193e-01, -5.4960e-01],\n",
      "         [-8.2327e-01,  8.4603e-01, -6.6348e-01,  ..., -2.0132e-01,\n",
      "           1.2730e+00, -8.0405e-01],\n",
      "         [-7.4022e-01,  6.8229e-01, -1.9328e-01,  ..., -2.9264e-01,\n",
      "           7.4103e-01, -9.3218e-01],\n",
      "         [ 7.7214e-01, -2.4004e-01, -8.4904e-01,  ...,  1.7395e-01,\n",
      "           1.1160e+00,  1.8534e+00],\n",
      "         [-8.3695e-01,  1.6836e-01, -7.3559e-01,  ..., -8.2297e-02,\n",
      "           8.2059e-02,  6.8131e-01]],\n",
      "\n",
      "        [[ 1.0112e-01,  6.1023e-04,  1.2280e+00,  ...,  1.4239e+00,\n",
      "          -1.0427e+00, -7.7041e-01],\n",
      "         [-1.9589e+00,  2.5010e-01, -3.8665e-01,  ...,  8.4582e-01,\n",
      "           5.6371e-01,  8.1461e-02],\n",
      "         [-2.0782e-01, -1.1903e+00,  8.4159e-01,  ..., -1.2821e+00,\n",
      "           6.5002e-01, -1.3746e+00],\n",
      "         [ 5.2558e-01,  4.8669e-01, -5.6087e-01,  ..., -5.1120e-01,\n",
      "           1.5828e-01,  1.1571e+00],\n",
      "         [-4.8366e-01,  9.3293e-01, -2.1450e+00,  ...,  5.4454e-02,\n",
      "           1.8185e+00,  2.0764e-02]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model = DummyGPT(GPT_config)\n",
    "logits = model(batch)\n",
    "print(f\"Output size:{logits.shape}\")\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer normalization\n",
    "To avoid problems such as vanishing or exploding gradients, we normalize the output of each layer before applying the activation function. This is done by applying a layer normalization operation to the output of each sub-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.1842, 0.0052, 0.7233, 0.0000, 0.5298],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2237, 0.0000, 0.7727]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_example = torch.randn(2,5)\n",
    "layer = nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here the relu activate function creates a theshold such that negative inputs are made 0, ensuring only positive values are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:tensor([[0.2404],\n",
      "        [0.1661]], grad_fn=<MeanBackward1>)\n",
      "variance:tensor([[0.2404],\n",
      "        [0.1661]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1,keepdim=True)\n",
    "var = out.mean(dim=-1,keepdim=True)\n",
    "print(f\"Mean:{mean}\")\n",
    "print(f\"variance:{var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now applying layer normalization to layer outputs obtained previously, we ensure that the output is scaled to a mean of 0 and variance of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:tensor([[-0.4903, -0.1147, -0.4798,  0.9849, -0.4903,  0.5902],\n",
      "        [-0.4075, -0.4075, -0.4075,  0.1415, -0.4075,  1.4886]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:tensor([[1.9868e-08],\n",
      "        [1.4901e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:tensor([[0.4085],\n",
      "        [0.5800]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out-mean)/torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1,keepdim=True)\n",
    "var = out_norm.var(dim=-1,keepdim=True)\n",
    "print(f\"Normalized layer outputs:{out_norm}\")\n",
    "print(f\"Mean:{mean}\")\n",
    "print(f\"Variance:{var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer normalization class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1,keepdim=True)\n",
    "        var = x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gelu activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return 0.5*x*(1+torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0/torch.pi))*\n",
    "            (x+0.044715+torch.pow(x,3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"],cfg[\"emb_dim\"]*4),\n",
    "            GELU(),\n",
    "            nn.Linear(cfg[\"emb_dim\"]*4,cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_config)\n",
    "x = torch.rand(2,3,768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding shortcut connections:\n",
    "\n",
    "The goal is to understand the impact of adding residual connections to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleDNN(nn.Module):\n",
    "    def __init__(self,layer_sizes,use_residual):\n",
    "        super().__init__()\n",
    "        self.use_residual = use_residual\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(layer_sizes[i],layer_sizes[i+1])\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_residual and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3,3,3,3,3,1]\n",
    "sample_input = torch.tensor([[1.,0.,-1.]])\n",
    "torch.manual_seed(42)\n",
    "non_residual_model = SampleDNN(layer_sizes,use_residual=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print gradients to check the impact of the residual connection\n",
    "def print_gradients(model,x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output,target)\n",
    "    loss.backward()\n",
    "    for name,param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name}:{param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight:0.010758275166153908\n",
      "layers.1.0.weight:0.02833496779203415\n",
      "layers.2.0.weight:0.031643934547901154\n",
      "layers.3.0.weight:0.12355596572160721\n",
      "layers.4.0.weight:0.18377549946308136\n"
     ]
    }
   ],
   "source": [
    "print_gradients(non_residual_model,sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight:0.18751747906208038\n",
      "layers.1.0.weight:0.16522449254989624\n",
      "layers.2.0.weight:0.21649211645126343\n",
      "layers.3.0.weight:0.25283390283584595\n",
      "layers.4.0.weight:0.9302193522453308\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(420)\n",
    "residual_model = SampleDNN(layer_sizes,use_residual=True)\n",
    "print_gradients(residual_model,sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While we do not see a stark difference in the output, we do see the gradients in the initial layers being stable and rsising gradually in the model with residual connections.\n",
    "- The residual connections help in training the model faster and also helps in avoiding the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,\n",
    "                 context_length,dropout,\n",
    "                 num_heads, qkv_bias=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out,d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length,context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        b, num_tokens , d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Splitting the matrix by adding num_heads\n",
    "        keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "\n",
    "\n",
    "        # Converting the shape\n",
    "        # b,num_tokens,num_heads,head_dim = b,num_heads,num_tokens,head_dim\n",
    "        # crucial for aligning the keys and values in multiple heads\n",
    "        keys = keys.transpose(1,2) \n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(2,3) # dot product for each head\n",
    "        mask_bool = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "        attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1]**0.5 , dim = -1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # b , num_token , n_heads , head_dim\n",
    "        context_vectors = (attention_weights @ values).transpose(1,2) \n",
    "\n",
    "        # combines the heads \n",
    "        context_vectors = context_vectors.contiguous().view(\n",
    "            b,num_tokens,self.d_out\n",
    "        )\n",
    "        context_vectors = self.out_proj(context_vectors)\n",
    "        return context_vectors\n",
    "    \n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out= cfg[\"emb_dim\"],\n",
    "            context_length= cfg[\"context_length\"],\n",
    "            num_heads= cfg[\"n_heads\"],\n",
    "            dropout= cfg[\"drop_rate\"],\n",
    "            qkv_bias= cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])   \n",
    "\n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)  \n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:torch.Size([2, 4, 768])\n",
      "output shape:torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "x = torch.rand(2,4,768)\n",
    "block = TransformerBlock(GPT_config)\n",
    "out = block(x)\n",
    "print(f\"input shape:{x.shape}\")\n",
    "print(f\"output shape:{out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for  _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self,in_idx):\n",
    "        batch_size , seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len,device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch:\n",
      "tensor([[16833,  1110,   318,   257,   922],\n",
      "        [ 1169,  6766, 32481,   290,   318]])\n",
      "output shape:torch.Size([2, 5, 50257])\n",
      "tensor([[[ 0.6484, -0.1046, -0.1722,  ..., -0.2517, -0.4366, -0.6187],\n",
      "         [-0.0685,  0.4742,  0.6509,  ...,  0.1216, -0.1821, -0.5249],\n",
      "         [ 0.3725,  1.2968,  0.4850,  ...,  0.6677,  0.6186, -0.6493],\n",
      "         [-0.2975, -0.2977,  1.1011,  ..., -0.2127, -0.2804, -0.9128],\n",
      "         [-0.3206,  0.0167,  0.1753,  ..., -0.1021, -0.5538,  0.1212]],\n",
      "\n",
      "        [[-0.1876, -0.0367,  1.3480,  ...,  0.1788,  0.5199,  0.1640],\n",
      "         [ 0.1850, -0.5256,  0.2818,  ...,  0.4252, -0.1914,  0.1537],\n",
      "         [ 0.1942,  0.7726,  0.0324,  ...,  0.8447, -0.4039, -0.3113],\n",
      "         [-1.0523, -0.5826,  0.1886,  ...,  0.9998, -0.1479, -1.0728],\n",
      "         [ 0.4366, -0.1486, -0.0560,  ...,  0.1409,  0.1826, -0.7229]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model = GPTModel(GPT_config)\n",
    "out = model(batch)\n",
    "print(f\"input batch:\\n{batch}\")\n",
    "print(f\"output shape:{out.shape}\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in GPT:163009536\n"
     ]
    }
   ],
   "source": [
    "total_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in GPT:{total_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 consists of 124M parameters.However here we see 163M parameters. The reason is that GPT-2 reuses the weights from the token embedding layer in the output layer. This process is called weight tying.\n",
    "This practice of removing the weights from the output layer helps in reducing the overall memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters in MultiHeadAttention:2,360,064\n",
      "Parameters in FeedForward:4,722,432\n"
     ]
    }
   ],
   "source": [
    "#checking the number of parameters in each individual module\n",
    "block = TransformerBlock(GPT_config)\n",
    "mha_params = sum(p.numel() for p in block.att.parameters())\n",
    "print(f\"Parameters in MultiHeadAttention:{mha_params:,}\")\n",
    "\n",
    "ff_params = sum(p.numel() for p in block.ff.parameters())\n",
    "print(f\"Parameters in FeedForward:{ff_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,idx,max_new_tokens,context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:,-1,:] \n",
    "        probas = torch.softmax(logits,dim=-1)\n",
    "        idx_next = torch.argmax(probas,dim=-1,keepdim=True)\n",
    "        idx = torch.cat((idx,idx_next),dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded text:[15496, 616, 1438]\n",
      "encoded tensors: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "example_context = \"Hello my name\"\n",
    "encoded = tokenizer.encode(example_context)\n",
    "print(f\"encoded text:{encoded}\")\n",
    "encoded_tensors = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded tensors:\",encoded_tensors.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,   616,  1438, 18612, 48670, 28246, 39567, 46805, 44013]])\n",
      "output length:9\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text(model,encoded_tensors,6,GPT_config[\"context_length\"])\n",
    "print(out)\n",
    "print(f\"output length:{len(out[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name Professional rg hemp Warn PROGRAM ABE\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
